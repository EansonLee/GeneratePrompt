import os
import requests
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.messages import HumanMessage
from typing import Tuple, List
import json
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.DEBUG)

def get_available_models(api_key: str, base_url: str) -> List[str]:
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    
    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
    
    Returns:
        List[str]: å¯ç”¨æ¨¡å‹åˆ—è¡¨
    """
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        print("\nğŸ“¡ è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
        response = requests.get(
            f"{base_url}/models",
            headers=headers,
            timeout=30
        )
        
        if response.status_code == 200:
            models = response.json().get("data", [])
            model_ids = [model["id"] for model in models]
            print("\nâœ… è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸï¼")
            print("\nå¯ç”¨æ¨¡å‹:")
            for model_id in model_ids:
                print(f"- {model_id}")
            return model_ids
        else:
            print(f"\nâŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼šçŠ¶æ€ç  {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return []
            
    except Exception as e:
        print(f"\nâŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
        return []

def test_openai_key() -> Tuple[bool, str]:
    """æµ‹è¯•OpenAI APIå¯†é’¥æ˜¯å¦å¯ç”¨
    
    Returns:
        Tuple[bool, str]: (å¯†é’¥æ˜¯å¦å¯ç”¨, è¯¦ç»†ä¿¡æ¯)
    """
    try:
        # ç›´æ¥è®¾ç½®APIé…ç½®
        api_key = "sk-FastAPIvE1M0Ktm0qjx1IZm4LIA1bVdR0mZ0aOtH3BCz2wjn"
        base_url = "https://api.fastapi.ai/v1"  # æ·»åŠ  /v1 åˆ°base_url
            
        print("\nğŸ”„ æ­£åœ¨æµ‹è¯•APIå¯†é’¥...")
        print(f"Base URL: {base_url}")
        print(f"API Key: {api_key}")

        # 1. è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        available_models = get_available_models(api_key, base_url)
        if not available_models:
            return False, "æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"

        # 2. å°è¯•ç›´æ¥è°ƒç”¨API
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
        model_name = "claude-3-7-sonnet-20250219"
        
        data = {
            "model": model_name,
            "messages": [{"role": "user", "content": "è¿”å›'æµ‹è¯•æˆåŠŸ'è¿™ä¸‰ä¸ªå­—"}],
            "temperature": 0
        }
        
        print("\nğŸ“¡ ç›´æ¥è°ƒç”¨APIæµ‹è¯•...")
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
        print(f"è¯·æ±‚å¤´: {json.dumps(headers, indent=2, ensure_ascii=False)}")
        print(f"è¯·æ±‚ä½“: {json.dumps(data, indent=2, ensure_ascii=False)}")
        
        response = requests.post(
            f"{base_url}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        print(f"\nğŸ“¥ APIå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {dict(response.headers)}")
        print(f"å“åº”å†…å®¹: {response.text}")

        if response.status_code == 200:
            print("\nâœ… ç›´æ¥APIè°ƒç”¨æµ‹è¯•æˆåŠŸï¼")
            
            # 3. ä½¿ç”¨LangChainæµ‹è¯•
            print("\nğŸ”„ ä½¿ç”¨LangChainæµ‹è¯•...")
            
            chat = ChatOpenAI(
                api_key=api_key,
                base_url=base_url,
                model_name=model_name,
                temperature=0,
                timeout=30,
                max_retries=3,
                verbose=True,
                streaming=False,
                default_headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key}"
                }
            )
            
            # å‘é€æµ‹è¯•æ¶ˆæ¯
            test_message = "è¿”å›'æµ‹è¯•æˆåŠŸ'è¿™ä¸‰ä¸ªå­—"
            messages = [HumanMessage(content=test_message)]
            
            print("ğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯...")
            response = chat.invoke(messages)
            
            # éªŒè¯å“åº”
            if response and response.content:
                print(f"ğŸ“¥ æ”¶åˆ°å“åº”: {response.content}")
                print("\nâœ… LangChainæµ‹è¯•æˆåŠŸï¼")
                return True, "APIæµ‹è¯•æˆåŠŸï¼ˆç›´æ¥è°ƒç”¨å’ŒLangChainéƒ½æˆåŠŸï¼‰"
            
        return False, f"APIæµ‹è¯•å¤±è´¥ï¼šçŠ¶æ€ç  {response.status_code}"
            
    except requests.exceptions.RequestException as e:
        error_msg = f"HTTPè¯·æ±‚é”™è¯¯: {str(e)}"
        print(f"\nâŒ {error_msg}")
        return False, error_msg
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ APIæµ‹è¯•å¤±è´¥ï¼š{error_msg}")
        return False, f"APIæµ‹è¯•å¤±è´¥ï¼š{error_msg}"

def test_embedding_model(api_key: str, base_url: str) -> Tuple[bool, str]:
    """æµ‹è¯•æ–‡æœ¬åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
    
    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
    
    Returns:
        Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, è¯¦ç»†ä¿¡æ¯)
    """
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "User-Agent": "OpenAI/v1 PythonClient/1.0.0"
        }
        
        data = {
            "model": "text-embedding-ada-002",
            "input": "æµ‹è¯•æ–‡æœ¬",
            "encoding_format": "float"
        }
        
        print("\nğŸ“¡ æµ‹è¯•æ–‡æœ¬åµŒå…¥æ¥å£...")
        print(f"è¯·æ±‚URL: {base_url}/embeddings")
        print(f"è¯·æ±‚å¤´: {json.dumps(headers, indent=2, ensure_ascii=False)}")
        print(f"è¯·æ±‚ä½“: {json.dumps(data, indent=2, ensure_ascii=False)}")
        
        response = requests.post(
            f"{base_url}/embeddings",
            headers=headers,
            json=data,
            timeout=30
        )
        
        print(f"\nğŸ“¥ å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {json.dumps(dict(response.headers), indent=2, ensure_ascii=False)}")
        
        if response.status_code == 200:
            response_data = response.json()
            print(f"å“åº”å†…å®¹æ‘˜è¦: {json.dumps({k: v for k, v in response_data.items() if k != 'data'}, indent=2, ensure_ascii=False)}")
            embedding_length = len(response_data.get('data', [{}])[0].get('embedding', []))
            print(f"åµŒå…¥å‘é‡ç»´åº¦: {embedding_length}")
            return True, f"åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œå‘é‡ç»´åº¦: {embedding_length}"
        else:
            try:
                error_data = response.json()
                error_msg = f"è¯·æ±‚å¤±è´¥: {json.dumps(error_data, indent=2, ensure_ascii=False)}"
            except:
                error_msg = f"è¯·æ±‚å¤±è´¥: HTTP {response.status_code}, {response.text}"
            print(f"âŒ {error_msg}")
            return False, error_msg
            
    except Exception as e:
        error_msg = f"æµ‹è¯•å¤±è´¥: {str(e)}"
        print(f"\nâŒ {error_msg}")
        if hasattr(e, '__traceback__'):
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False, error_msg

def test_embedding_with_langchain(api_key: str, base_url: str) -> Tuple[bool, str]:
    """ä½¿ç”¨ LangChain API æµ‹è¯•æ–‡æœ¬åµŒå…¥æ¨¡å‹
    
    Args:
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
    
    Returns:
        Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, è¯¦ç»†ä¿¡æ¯)
    """
    try:
        print("\nğŸ“¡ ä½¿ç”¨ LangChain API æµ‹è¯•æ–‡æœ¬åµŒå…¥...")
        
        # åˆå§‹åŒ– OpenAIEmbeddings
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=api_key,
            openai_api_base=base_url,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "OpenAI/v1 PythonClient/1.0.0"
            },
            timeout=30,
            max_retries=3,
            model_kwargs={
                "encoding_format": "float"
            }
        )
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = ["è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", "è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æœ¬"]
        test_query = "æµ‹è¯•æŸ¥è¯¢æ–‡æœ¬"
        
        print("\nğŸ”„ æµ‹è¯• embed_documents...")
        try:
            # æµ‹è¯•æ–‡æ¡£åµŒå…¥
            doc_embeddings = embeddings.embed_documents(test_texts)
            print(f"æ–‡æ¡£åµŒå…¥æˆåŠŸï¼ç»´åº¦: {len(doc_embeddings[0])}")
            print(f"åµŒå…¥æ•°é‡: {len(doc_embeddings)}")
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åµŒå…¥å¤±è´¥: {str(e)}")
            raise
            
        print("\nğŸ”„ æµ‹è¯• embed_query...")
        try:
            # æµ‹è¯•æŸ¥è¯¢åµŒå…¥
            query_embedding = embeddings.embed_query(test_query)
            print(f"æŸ¥è¯¢åµŒå…¥æˆåŠŸï¼ç»´åº¦: {len(query_embedding)}")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢åµŒå…¥å¤±è´¥: {str(e)}")
            raise
            
        # æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦
        print("\nğŸ”„ æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦è®¡ç®—...")
        try:
            import numpy as np
            similarity = np.dot(doc_embeddings[0], query_embedding) / (
                np.linalg.norm(doc_embeddings[0]) * np.linalg.norm(query_embedding)
            )
            print(f"ç›¸ä¼¼åº¦è®¡ç®—æˆåŠŸï¼ç›¸ä¼¼åº¦åˆ†æ•°: {similarity}")
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            raise
            
        return True, "LangChain Embeddings API æµ‹è¯•å…¨éƒ¨æˆåŠŸï¼"
        
    except Exception as e:
        error_msg = f"LangChain API æµ‹è¯•å¤±è´¥: {str(e)}"
        print(f"\nâŒ {error_msg}")
        if hasattr(e, '__traceback__'):
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False, error_msg

def test_vision_model(api_key: str = None, base_url: str = None) -> Tuple[bool, str]:
    """æµ‹è¯• GPT-4 Vision æ¨¡å‹æ˜¯å¦å¯ç”¨
    
    Args:
        api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        base_url: APIåŸºç¡€URLï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
    
    Returns:
        Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, è¯¦ç»†ä¿¡æ¯)
    """
    try:
        # ä½¿ç”¨é»˜è®¤é…ç½®
        if api_key is None:
            api_key = "sk-FastAPIvE1M0Ktm0qjx1IZm4LIA1bVdR0mZ0aOtH3BCz2wjn"
        if base_url is None:
            base_url = "https://api.fastapi.ai/v1"
            
        print("\nğŸ“¡ æµ‹è¯• GPT-4 Vision æ¨¡å‹...")
        print(f"Base URL: {base_url}")
        print(f"API Key: {api_key}")
        
        # å‡†å¤‡æµ‹è¯•å›¾ç‰‡ï¼ˆä½¿ç”¨base64ç¼–ç çš„ç®€å•å›¾ç‰‡æ•°æ®ï¼‰
        test_image = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg=="
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "application/json",
            "User-Agent": "OpenAI/v1 PythonClient/1.0.0"
        }
        
        # é¦–å…ˆè·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        print("\nğŸ”„ è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
        models_response = requests.get(
            f"{base_url}/models",
            headers=headers,
            timeout=30,
            verify=True
        )
        
        if models_response.status_code == 200:
            models = models_response.json().get("data", [])
            model_ids = [model["id"] for model in models]
            print("\nå¯ç”¨æ¨¡å‹åˆ—è¡¨:")
            for model_id in model_ids:
                print(f"- {model_id}")
            
            # é€‰æ‹©åˆé€‚çš„æ¨¡å‹
            vision_model = None
            for model_id in model_ids:
                if "vision" in model_id.lower() or "gpt-4" in model_id.lower():
                    vision_model = model_id
                    break
            
            if not vision_model:
                return False, "æœªæ‰¾åˆ°å¯ç”¨çš„ Vision æ¨¡å‹"
        else:
            print(f"\nâŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {models_response.status_code}")
            vision_model = "gpt-4-vision-preview"  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
        
        print(f"\nä½¿ç”¨æ¨¡å‹: {vision_model}")
        
        data = {
            "model": "gpt-4o",
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å›¾ç‰‡ï¼Œè¯·è¿”å›'æµ‹è¯•æˆåŠŸ'è¿™ä¸‰ä¸ªå­—"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{test_image}"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 300,
            "temperature": 0
        }
        
        url = f"{base_url}/chat/completions"
        print(f"\nè¯·æ±‚URL: {url}")
        print(f"è¯·æ±‚å¤´: {json.dumps(headers, indent=2, ensure_ascii=False)}")
        print(f"è¯·æ±‚ä½“: {json.dumps(data, indent=2, ensure_ascii=False)}")
        
        response = requests.post(
            url,
            headers=headers,
            json=data,
            timeout=30,
            verify=True
        )
        
        print(f"\nğŸ“¥ å“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´: {dict(response.headers)}")
        print(f"å“åº”å†…å®¹: {response.text}")
        
        if response.status_code == 200:
            response_data = response.json()
            if 'choices' in response_data and len(response_data['choices']) > 0:
                content = response_data['choices'][0].get('message', {}).get('content', '')
                print(f"\næ¨¡å‹è¿”å›å†…å®¹: {content}")
                print("\nâœ… GPT-4 Vision æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
                return True, "GPT-4 Vision æ¨¡å‹æµ‹è¯•æˆåŠŸ"
            else:
                error_msg = "å“åº”æ ¼å¼ä¸æ­£ç¡®"
                print(f"\nâŒ {error_msg}")
                return False, error_msg
        else:
            try:
                error_data = response.json()
                error_msg = f"GPT-4 Vision æ¨¡å‹æµ‹è¯•å¤±è´¥: HTTP {response.status_code}, {error_data.get('error', {}).get('message', 'æœªçŸ¥é”™è¯¯')}"
            except:
                error_msg = f"GPT-4 Vision æ¨¡å‹æµ‹è¯•å¤±è´¥: HTTP {response.status_code}"
            print(f"\nâŒ {error_msg}")
            return False, error_msg
            
    except Exception as e:
        error_msg = f"GPT-4 Vision æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"
        print(f"\nâŒ {error_msg}")
        if hasattr(e, '__traceback__'):
            import traceback
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        return False, error_msg

if __name__ == "__main__":
    # è®¾ç½®è¯¦ç»†çš„HTTPè¯·æ±‚æ—¥å¿—
    import http.client as http_client
    http_client.HTTPConnection.debuglevel = 1
    
    # é…ç½®requestsçš„æ—¥å¿—
    import logging
    logging.basicConfig(level=logging.DEBUG)
    requests_log = logging.getLogger("requests.packages.urllib3")
    requests_log.setLevel(logging.DEBUG)
    requests_log.propagate = True
    
    # APIé…ç½®
    api_key = "sk-FastAPIvE1M0Ktm0qjx1IZm4LIA1bVdR0mZ0aOtH3BCz2wjn"
    base_url = "https://api.fastapi.ai/v1"
    
    # è¿è¡Œå¸¸è§„APIæµ‹è¯•
    print("\n=== æµ‹è¯•å¸¸è§„API ===")
    success, message = test_openai_key()
    print(f"\nå¸¸è§„APIæµ‹è¯•ç»“æœ: {message}")
    
    # è¿è¡Œç›´æ¥åµŒå…¥æ¨¡å‹æµ‹è¯•
    print("\n=== æµ‹è¯•ç›´æ¥è°ƒç”¨åµŒå…¥æ¨¡å‹ ===")
    embedding_success, embedding_message = test_embedding_model(api_key, base_url)
    print(f"\nç›´æ¥è°ƒç”¨åµŒå…¥æ¨¡å‹æµ‹è¯•ç»“æœ: {embedding_message}")
    
    # è¿è¡Œ LangChain API æµ‹è¯•
    print("\n=== æµ‹è¯• LangChain API ===")
    langchain_success, langchain_message = test_embedding_with_langchain(api_key, base_url)
    print(f"\nLangChain API æµ‹è¯•ç»“æœ: {langchain_message}")
    
    # è¿è¡Œ GPT-4 Vision æ¨¡å‹æµ‹è¯•
    print("\n=== æµ‹è¯• GPT-4 Vision æ¨¡å‹ ===")
    vision_success, vision_message = test_vision_model(api_key, base_url)
    print(f"\nGPT-4 Vision æ¨¡å‹æµ‹è¯•ç»“æœ: {vision_message}")

    # æ ¹æ®æµ‹è¯•ç»“æœå†³å®šé€€å‡ºç 
    exit(0 if vision_success else 1) 